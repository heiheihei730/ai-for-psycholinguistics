{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 其他语言任务\n",
    "\n",
    "在本小节中，我们以主题分析任务和上下文学习为例，演示语言模型的加载和推理过程。对于其他语言任务，均可在huggingface平台搜索到类似的教程文档以及代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from transformers import (\n",
    "    BertTokenizer,\n",
    "    GPT2LMHeadModel, \n",
    "    TextGenerationPipeline,\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification, \n",
    "    AutoModelForSeq2SeqLM,\n",
    "    pipeline\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 主题分析任务\n",
    "使用transformers管道pipeline快速实现语言任务"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== 单个句子主题分析计算 ====================\n",
      "\n",
      "Input: 欢迎参加工作坊！\n",
      "Prediction: culture, Score: 0.723\n",
      "\n",
      "\n",
      "\n",
      "==================== 多个句子批量进行主题分析计算 ====================\n",
      "\n",
      "Input: 2023年心理语言学会在广州召开\n",
      "Prediction: culture, Score: 0.969\n",
      "\n",
      "Input: 湖人有意签保罗补强，联手詹姆斯追逐总冠军\n",
      "Prediction: sports, Score: 1.000\n"
     ]
    }
   ],
   "source": [
    "# 从huggingface平台上找到对应的模型路径\n",
    "model_path = 'model/roberta-base-finetuned-chinanews-chinese'\n",
    "\n",
    "# 使用transformers工具包加载模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "\n",
    "# 利用pipeline快速进行语言任务\n",
    "text = '欢迎参加工作坊！'\n",
    "text_classification = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)\n",
    "res = text_classification(text)[0]\n",
    "print(\"=\"*20, \"单个句子主题分析计算\", \"=\"*20)\n",
    "print(f\"\\nInput: {text}\\nPrediction: {res['label']}, Score: {res['score']:.3f}\")\n",
    "\n",
    "\n",
    "# pipeline可以实现批量句子的计算\n",
    "text_lst = ['2023年心理语言学会在广州召开', '湖人有意签保罗补强，联手詹姆斯追逐总冠军']\n",
    "res_lst = text_classification(text_lst)\n",
    "print(\"\\n\\n\")\n",
    "print(\"=\"*20, \"多个句子批量进行主题分析计算\", \"=\"*20)\n",
    "for text, res in zip(text_lst, res_lst):\n",
    "    print(f\"\\nInput: {text}\\nPrediction: {res['label']}, Score: {res['score']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 上下文学习\n",
    "通过在上下文中给定任务描述和示例，通用的文本生成模型可以根据上下文快速学习语言任务。在这里我们不使用pipeline，直接调用模型方法进行计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "==================== 上下文学习实现文本翻译 ====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhang/anaconda3/envs/ngram/lib/python3.7/site-packages/transformers/generation/utils.py:1278: UserWarning: Neither `max_length` nor `max_new_tokens` has been set, `max_length` will default to 20 (`generation_config.max_length`). Controlling `max_length` via the config is deprecated and `max_length` will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  UserWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: translate English to German: How old are you?\n",
      "Output: Wie alte sind Sie?\n",
      "\n",
      "\n",
      "\n",
      "==================== 上下文学习实现主题文本生成 ====================\n",
      "Input: Generate sentences with the topic : \n",
      "sports => Lionel Messi and MLS club Inter Miami are discussing possible signing\n",
      "entertainment => \n",
      "\n",
      "Output: a new tv series starring adrian sandler is \n"
     ]
    }
   ],
   "source": [
    "# 从huggingface平台上找到对应的模型路径\n",
    "model_path = \"model/flan-t5-large\"\n",
    "\n",
    "# 使用transformers工具包加载模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"=\"*20, \"上下文学习实现文本翻译\", \"=\"*20)\n",
    "text = \"translate English to German: How old are you?\"\n",
    "\n",
    "# 调用模型分词器，对输入文本进行分词并转换为模型可处理的tensor形式\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# 调用模型的generate方法\n",
    "outputs = model.generate(input_ids)\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens = True)\n",
    "print(f\"Input: {text}\\nOutput: {decoded_output}\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "print(\"=\"*20, \"上下文学习实现主题文本生成\", \"=\"*20)\n",
    "text = '''Generate sentences with the topic : \n",
    "sports => Lionel Messi and MLS club Inter Miami are discussing possible signing\n",
    "entertainment => \n",
    "'''\n",
    "\n",
    "# 调用模型分词器，对输入文本进行分词并转换为模型可处理的tensor形式\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# 调用模型的generate方法\n",
    "outputs = model.generate(input_ids)\n",
    "decoded_output = tokenizer.decode(outputs[0], skip_special_tokens = True)\n",
    "print(f\"Input: {text}\\nOutput: {decoded_output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 文本生成超参数\n",
    "在本小节中，我们会分析文本生成中的温度参数、搜索策略参数以及top-p参数对文本生成结果的影响。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: Welcome to \n",
      "\n",
      "==================== 贪婪搜索 ====================\n",
      "Iter 0: Welcome to the e-commerce world!\n",
      "Iter 1: Welcome to the e-commerce world!\n",
      "Iter 2: Welcome to the e-commerce world!\n",
      "Iter 3: Welcome to the e-commerce world!\n",
      "Iter 4: Welcome to the e-commerce world!\n",
      "==================== 随机搜索, 温度参数=0.1 ====================\n",
      "Iter 0: Welcome to the iStockphoto\n",
      "Iter 1: Welcome to the official website of the \n",
      "Iter 2: Welcome to the world of e-commerce\n",
      "Iter 3: Welcome to the world of e-commerce\n",
      "Iter 4: Welcome to the official website of the \n",
      "==================== 随机搜索, 温度参数=1.0 ====================\n",
      "Iter 0: Welcome to World of Aliens! a\n",
      "Iter 1: Welcome to the new website.  All\n",
      "Iter 2: Welcome to the Wikimedia Foundation! This\n",
      "Iter 3: Hi, I am Jason, the owners of\n",
      "Iter 4: Welcome to the Official Website of eW\n"
     ]
    }
   ],
   "source": [
    "# 从huggingface平台上找到对应的模型路径\n",
    "model_path = \"model/flan-t5-large\"\n",
    "\n",
    "# 使用transformers工具包加载模型\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
    "\n",
    "text = 'Welcome to '\n",
    "\n",
    "# 调用模型分词器，对输入文本进行分词并转换为模型可处理的tensor形式\n",
    "input_ids = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# 其余可修改参数包括top_k, top_p等, 可直接在.generate()方法中调用\n",
    "# ref: https://huggingface.co/blog/how-to-generate\n",
    "print(f'\\nInput: {text}\\n')\n",
    "print(\"=\"*20, \"贪婪搜索\", \"=\"*20)\n",
    "for iter in range(5):\n",
    "    outputs = model.generate(input_ids, max_length=10)\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens = True)\n",
    "    print(f\"Iter {iter}: {decoded_output}\")\n",
    "    \n",
    "print(\"=\"*20, \"随机搜索, 温度参数=0.1\", \"=\"*20)\n",
    "for iter in range(5):\n",
    "    outputs = model.generate(input_ids, do_sample=True, temperature=0.1, max_length=10)\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens = True)\n",
    "    print(f\"Iter {iter}: {decoded_output}\")\n",
    "    \n",
    "\n",
    "print(\"=\"*20, \"随机搜索, 温度参数=1.0\", \"=\"*20)\n",
    "for iter in range(5):\n",
    "    outputs = model.generate(input_ids, do_sample=True, temperature=1.0, max_length=10)\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens = True)\n",
    "    print(f\"Iter {iter}: {decoded_output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "heiheihei",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
